{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac749356-efe5-4e7a-8ffa-a2bd1fbcec3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edb297-b5dd-47f0-9aaa-6065c24c42ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"script_v2-151_2A1_primary-modified[dot]ipynb\"\n",
    "\n",
    "# Script \"v2.151\" (2A1): WGAN-SN Model Definition & Training (Enhanced Edition)\n",
    "# With added metrics (FID/KID) and visualization capabilities\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import logging \n",
    "import traceback \n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "\n",
    "# --- Auto-installation Block ---\n",
    "def install_and_import(package_name, import_name=None, pip_name=None):\n",
    "    \"\"\"Tries to import a package, installs it via pip if import fails.\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    if pip_name is None:\n",
    "        pip_name = package_name\n",
    "    try:\n",
    "        module = importlib.import_module(package_name)\n",
    "        globals()[import_name] = module\n",
    "        print(f\"Successfully imported {package_name} as {import_name}\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"{package_name} not found. Attempting installation using pip...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "            module = importlib.import_module(package_name)\n",
    "            globals()[import_name] = module\n",
    "            print(f\"Successfully installed and imported {package_name} as {import_name}\")\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, ImportError, ModuleNotFoundError) as e:\n",
    "            print(f\"ERROR: Failed to install/import {package_name} (pip name: {pip_name}). {e}\")\n",
    "            print(\"Please install required packages manually and restart kernel.\")\n",
    "            return False\n",
    "\n",
    "print(\"--- Checking and Installing Dependencies ---\")\n",
    "numpy_success = install_and_import('numpy', 'np')\n",
    "torch_success = install_and_import('torch')\n",
    "torchvision_success = install_and_import('torchvision')\n",
    "install_and_import('PIL')\n",
    "install_and_import('tqdm')\n",
    "install_and_import('matplotlib.pyplot', 'plt') # changed to resolve a function referncing error\n",
    "install_and_import('scipy')\n",
    "install_and_import('pytorch_fid', pip_name='pytorch-fid')\n",
    "install_and_import('pynvml', pip_name='nvidia-ml-py3')\n",
    "# New dependency for t-SNE visualization\n",
    "install_and_import('sklearn.manifold', 'manifold', pip_name='scikit-learn')\n",
    "\n",
    "# Check critical dependencies\n",
    "critical_imports_successful = all([numpy_success, torch_success, torchvision_success])\n",
    "if not critical_imports_successful:\n",
    "    print(\"ERROR: Critical packages (numpy, torch, torchvision) failed to import.\")\n",
    "    print(\"Please install these packages manually and restart the script.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Core Imports ---\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn.utils import spectral_norm \n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import FID calculation utilities\n",
    "try:\n",
    "    from pytorch_fid.inception import InceptionV3\n",
    "    from pytorch_fid.fid_score import calculate_frechet_distance\n",
    "    # Import KID calculation functions if available\n",
    "    try:\n",
    "        from pytorch_fid.kid_score import polynomial_kernel, calculate_kid_given_features\n",
    "        KID_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"WARNING: KID calculation not available in pytorch_fid. Implementing custom KID calculation.\")\n",
    "        KID_AVAILABLE = True  # We'll implement it ourselves if not in library\n",
    "        \n",
    "    FID_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: Could not import FID utilities from 'pytorch_fid'. FID/KID calculation will be disabled.\")\n",
    "    FID_AVAILABLE = False\n",
    "    KID_AVAILABLE = False\n",
    "\n",
    "# --- Import models and dataset class from separate files ---\n",
    "try:\n",
    "    from pollen_datasets_v2A1 import PollenDataset \n",
    "    from wgan_models_v2A1 import Generator, CriticSN, initialize_weights \n",
    "    print(\"Successfully imported models and dataset classes from local .py files.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Could not import classes from .py files: {e}\")\n",
    "    print(\"Ensure pollen_datasets_v2A1.py and wgan_models_v2A1.py exist in the same directory.\")\n",
    "    raise \n",
    "\n",
    "# --- Check NVML Availability ---\n",
    "NVML_AVAILABLE = False\n",
    "if 'pynvml' in globals():\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        device_count = pynvml.nvmlDeviceGetCount()\n",
    "        print(f\"Successfully initialized NVML. Found {device_count} NVIDIA GPU(s).\")\n",
    "        NVML_AVAILABLE = True\n",
    "        try: pynvml.nvmlShutdown()\n",
    "        except pynvml.NVMLError: print(\"Warning: NVML shutdown failed after check (might be harmless).\")\n",
    "    except pynvml.NVMLError as nvml_err:\n",
    "        print(f\"Warning: pynvml library imported but failed to initialize. GPU monitoring disabled.\")\n",
    "        print(f\"NVML Error: {nvml_err}\")\n",
    "        NVML_AVAILABLE = False\n",
    "else:\n",
    "     print(\"Warning: pynvml library not imported/installed. GPU monitoring disabled.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Configuration Section --- (EDIT THESE VALUES) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Paths ---\n",
    "# FIX: Corrected path to match the actual directory name\n",
    "PREPROCESSED_DATA_DIR = r\"C:\\Users\\praam\\Desktop\\havetai+vetcyto\\task-05_dataset\\pre-processing_px-128_step_automated-labels_pc-150\" \n",
    "OUTPUT_DIR = r\"C:\\Users\\praam\\Desktop\\havetai+vetcyto\\task-05_dataset\\WGAN-SN_training-output_v2-151\" \n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "IMAGE_SIZE = 128       \n",
    "CHANNELS_IMG = 1       \n",
    "NOISE_DIM = 100        \n",
    "G_FEATURES = 64        \n",
    "C_FEATURES = 64        \n",
    "\n",
    "# --- Training Hyperparameters --- \n",
    "LEARNING_RATE = 0.00005  # Updated lower learning rate, again back to 0.00005 from 0.00002\n",
    "BETA1 = 0.0            \n",
    "BETA2 = 0.9            \n",
    "BATCH_SIZE = 64        \n",
    "NUM_EPOCHS = 250       # Updated to 250 from 100 epochs\n",
    "CRITIC_ITERATIONS = 5  # Updated to 5 from 3\n",
    "\n",
    "# --- Checkpointing & Resuming ---\n",
    "CHECKPOINT_FREQ_EPOCHS = 5 \n",
    "RESUME_TRAINING = False   \n",
    "CHECKPOINT_FILE = \"latest_checkpoint_sn_v2151.pth.tar\" \n",
    "#BEST_FID_CHECKPOINT_TPL = \"best_fid_checkpoint_e{epoch:04d}_fid{fid:.2f}_v2151.pth.tar\"\n",
    "#BEST_KID_CHECKPOINT_TPL = \"best_kid_checkpoint_e{epoch:04d}_kid{kid:.2f}_v2151.pth.tar\"\n",
    "BEST_FID_CHECKPOINT_FILE = \"best_fid_checkpoint_v2151.pth.tar\"  # Fixed name for overwriting\n",
    "BEST_KID_CHECKPOINT_FILE = \"best_kid_checkpoint_v2151.pth.tar\"  # Fixed name for overwriting\n",
    "BEST_MODEL_FILE = \"best_model_v2151.pt\"\n",
    "\n",
    "# --- Logging & Monitoring ---\n",
    "LOG_FILE = \"training_log_sn_v2151.log\" \n",
    "SAMPLE_FREQ_STEPS = 500    \n",
    "MONITOR_TEMP = True        \n",
    "GPU_TEMP_THRESHOLD = 89    # Updated to 89C\n",
    "GPU_ID = 0                 \n",
    "TRACK_MEMORY_USAGE = True\n",
    "\n",
    "# --- FID/KID Calculation ---\n",
    "CALCULATE_FID = True\n",
    "CALCULATE_KID = True                   # New: Calculate KID alongside FID\n",
    "PRIMARY_EVAL_METRIC = \"FID\"            # New: Choose \"FID\" or \"KID\" for early stopping\n",
    "KID_SUBSET_SIZE = 1000                 # New: Subset size for KID calculation\n",
    "KID_SUBSETS = 100                      # New: Number of subsets for KID calculation\n",
    "FID_FREQ_EPOCHS = 1                    # Check every epoch\n",
    "FID_NUM_IMAGES = 10000                 # Use 10k images\n",
    "FID_BATCH_SIZE = 64\n",
    "REAL_STATS_PATH = os.path.join(OUTPUT_DIR, \"real_fid_stats_10k.npz\")\n",
    "#REAL_FEATURES_PATH = os.path.join(OUTPUT_DIR, \"real_inception_features_10k.npy\")  # New: Path to save real features\n",
    "REAL_FEATURES_PATH = os.path.join(OUTPUT_DIR, \"real_inception_features_10k.npy\")  # New: Path to save real features\n",
    "BEST_FEATURES_PATH = os.path.join(OUTPUT_DIR, \"best_fake_features_10k.npy\")  # New: Path to save best fake features\n",
    "FORCE_RECALCULATE_REAL_STATS = False\n",
    "USE_EARLY_STOPPING = True\n",
    "EARLY_STOPPING_PATIENCE = 200           # Changed to 25 from 10 & to 200 from 25\n",
    "\n",
    "# --- T-SNE Visualization ---                   # New section\n",
    "VISUALIZE_TSNE = True                          # Enable t-SNE visualization\n",
    "TSNE_SAMPLE_SIZE = 2000                        # Max number of samples to use for t-SNE (to keep computation manageable)\n",
    "TSNE_PERPLEXITY = 30                           # t-SNE perplexity parameter\n",
    "TSNE_RANDOM_STATE = 42                         # Random seed for reproducibility\n",
    "\n",
    "# --- Reproducibility ---\n",
    "MANUAL_SEED = 42\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Setup ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Create output directories ---\n",
    "CHKPT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints\")\n",
    "SAMPLE_DIR = os.path.join(OUTPUT_DIR, \"samples\")\n",
    "LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\")\n",
    "PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "ANALYSIS_DIR = os.path.join(OUTPUT_DIR, \"analysis_results\")\n",
    "\n",
    "# Create all required directories\n",
    "for directory in [CHKPT_DIR, SAMPLE_DIR, LOG_DIR, PLOT_DIR, ANALYSIS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# --- Setup Logging --- \n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(name)s - %(message)s')\n",
    "logger = logging.getLogger(\"WGAN_SN_Trainer_v2151\")\n",
    "logger.setLevel(logging.INFO) \n",
    "if logger.hasHandlers(): logger.handlers.clear() \n",
    "# File Handler\n",
    "file_handler = logging.FileHandler(os.path.join(LOG_DIR, LOG_FILE), mode='a') \n",
    "file_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "# Console Handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Starting WGAN-SN Training (v2.151) at {datetime.now()}\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"Logger configured.\")\n",
    "\n",
    "# --- Set Seed ---\n",
    "if MANUAL_SEED is not None:\n",
    "    logger.info(f\"Using manual seed: {MANUAL_SEED}\")\n",
    "    random.seed(MANUAL_SEED)\n",
    "    np.random.seed(MANUAL_SEED)\n",
    "    torch.manual_seed(MANUAL_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(MANUAL_SEED)\n",
    "else:\n",
    "    logger.info(\"Using random seed.\")\n",
    "\n",
    "# --- Memory Utility ---\n",
    "def log_gpu_memory_usage(step=''):\n",
    "    if not torch.cuda.is_available() or not TRACK_MEMORY_USAGE:\n",
    "        return\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # GB\n",
    "    reserved = torch.cuda.memory_reserved() / (1024 ** 3)    # GB\n",
    "    \n",
    "    logger.info(f\"GPU Memory [{step}]: Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# --- NVML Utility (Enhanced with device validation) ---\n",
    "def get_gpu_temp(gpu_id_to_check):\n",
    "    if not NVML_AVAILABLE: return None\n",
    "    temp = None\n",
    "    nvml_init_success = False\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "        nvml_init_success = True\n",
    "        \n",
    "        # Verify device count before accessing a specific index\n",
    "        device_count = pynvml.nvmlDeviceGetCount()\n",
    "        if gpu_id_to_check >= device_count:\n",
    "            logger.warning(f\"GPU ID {gpu_id_to_check} out of range (max: {device_count-1}). Using GPU 0.\")\n",
    "            gpu_id_to_check = 0\n",
    "            \n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id_to_check)\n",
    "        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "    except pynvml.NVMLError as nvml_err:\n",
    "        logger.warning(f\"Could not get GPU temp (ID {gpu_id_to_check}): {nvml_err}\", exc_info=False) \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Non-NVML error getting GPU temp: {e}\", exc_info=False) \n",
    "    finally:\n",
    "        if nvml_init_success:\n",
    "            try: pynvml.nvmlShutdown() \n",
    "            except pynvml.NVMLError: pass \n",
    "    return temp\n",
    "\n",
    "# --- Directory Validation Function ---\n",
    "def validate_directory(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        logger.error(f\"Directory not found: {dir_path}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# --- Checkpoint Utilities ---\n",
    "def save_checkpoint(state, filename):\n",
    "    save_path = os.path.join(CHKPT_DIR, filename)\n",
    "    logger.info(f\"=> Saving checkpoint to {save_path}\")\n",
    "    try:\n",
    "        torch.save(state, save_path)\n",
    "        logger.info(f\"   Checkpoint saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"!! Failed to save checkpoint: {e}\", exc_info=True) \n",
    "\n",
    "def load_checkpoint(filename, generator, critic, opt_gen, opt_critic, scaler_gen, scaler_critic): \n",
    "    load_path = os.path.join(CHKPT_DIR, filename)\n",
    "    logger.info(f\"=> Attempting to load checkpoint from {load_path}\")\n",
    "    if not os.path.exists(load_path):\n",
    "         logger.warning(f\"=> No checkpoint found at '{load_path}'. Starting from scratch.\")\n",
    "         return 0, 0, [], [], [], [], [], [], [], float('inf'), float('inf'), 0\n",
    "    try:\n",
    "        checkpoint = torch.load(load_path, map_location=torch.device('cpu')) \n",
    "        start_epoch = checkpoint.get('epoch', 0) \n",
    "        global_step = checkpoint.get('step', 0)\n",
    "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "        opt_gen.load_state_dict(checkpoint['optimizer_gen_state_dict'])\n",
    "        opt_critic.load_state_dict(checkpoint['optimizer_critic_state_dict'])\n",
    "        if 'scaler_gen_state_dict' in checkpoint and scaler_gen is not None:\n",
    "            scaler_gen.load_state_dict(checkpoint['scaler_gen_state_dict'])\n",
    "            logger.info(\"   Loaded GradScaler state for Generator.\")\n",
    "        else:\n",
    "            logger.warning(\"   Generator GradScaler state not found in checkpoint.\")\n",
    "        if 'scaler_critic_state_dict' in checkpoint and scaler_critic is not None:\n",
    "            scaler_critic.load_state_dict(checkpoint['scaler_critic_state_dict'])\n",
    "            logger.info(\"   Loaded GradScaler state for Critic.\")\n",
    "        else:\n",
    "            logger.warning(\"   Critic GradScaler state not found in checkpoint.\")\n",
    "        \n",
    "        g_losses_hist = checkpoint.get('g_losses_history', [])\n",
    "        c_losses_hist = checkpoint.get('c_losses_history', [])\n",
    "        fid_scores_hist = checkpoint.get('fid_scores_history', [])\n",
    "        fid_epochs_hist = checkpoint.get('fid_epochs_history', [])\n",
    "        kid_scores_hist = checkpoint.get('kid_scores_history', [])  \n",
    "        kid_std_hist = checkpoint.get('kid_std_history', [])        \n",
    "        kid_epochs_hist = checkpoint.get('kid_epochs_history', [])   \n",
    "        best_fid_val = checkpoint.get('best_fid', float('inf'))\n",
    "        best_kid_val = checkpoint.get('best_kid', float('inf'))     \n",
    "        epochs_no_improve_val = checkpoint.get('epochs_no_improve', 0)\n",
    "        \n",
    "        logger.info(f\"=> Loaded checkpoint successfully. Resuming from Epoch {start_epoch}, Step {global_step}\")\n",
    "        logger.info(f\"   History: {len(g_losses_hist)} loss points, {len(fid_scores_hist)} FID points, {len(kid_scores_hist)} KID points.\")\n",
    "        logger.info(f\"   Best FID: {best_fid_val:.4f}, Best KID: {best_kid_val:.4f}. Patience: {epochs_no_improve_val}\")\n",
    "        return start_epoch, global_step, g_losses_hist, c_losses_hist, fid_scores_hist, fid_epochs_hist, kid_scores_hist, kid_std_hist, kid_epochs_hist, best_fid_val, best_kid_val, epochs_no_improve_val\n",
    "    except KeyError as e:\n",
    "         logger.error(f\"=> Error loading checkpoint: Missing key {e}. Checkpoint might be incompatible or corrupt. Starting from scratch.\")\n",
    "         return 0, 0, [], [], [], [], [], [], [], float('inf'), float('inf'), 0\n",
    "    except Exception as e:\n",
    "        logger.error(f\"=> Error loading checkpoint: {e}. Starting from scratch.\", exc_info=True) \n",
    "        return 0, 0, [], [], [], [], [], [], [], float('inf'), float('inf'), 0\n",
    "\n",
    "# --- Save Best Model in Loadable Format ---\n",
    "def save_best_model(generator, filename=BEST_MODEL_FILE):\n",
    "    save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "    logger.info(f\"Saving best model to {save_path}\")\n",
    "    try:\n",
    "        # Save the model in a format suitable for loading in inference mode\n",
    "        torch.save({\n",
    "            'model_state_dict': generator.state_dict(),\n",
    "            'model_config': {\n",
    "                'noise_dim': NOISE_DIM,\n",
    "                'channels_img': CHANNELS_IMG,\n",
    "                'features_g': G_FEATURES\n",
    "            }\n",
    "        }, save_path)\n",
    "        logger.info(f\"Best model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save best model: {e}\", exc_info=True)\n",
    "\n",
    "# --- Save features from best model checkpoint ---\n",
    "def save_fake_features(fake_features, path=BEST_FEATURES_PATH):\n",
    "    \"\"\"Save fake features from the current best checkpoint\"\"\"\n",
    "    if fake_features is None:\n",
    "        logger.warning(\"No fake features to save.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Save features for later visualization\n",
    "        np.save(path, fake_features)\n",
    "        logger.info(f\"Saved best fake features to: {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save fake features: {e}\", exc_info=True)\n",
    "\n",
    "# --- Plotting Utilities ---\n",
    "# Original plot_metrics function is kept for backward compatibility but not used\n",
    "def plot_metrics(g_losses, c_losses, fid_scores, fid_epochs, save_dir):\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot losses\n",
    "        plt.subplot(1, 2, 1)\n",
    "        epochs = range(1, len(g_losses) + 1)\n",
    "        if g_losses:\n",
    "            plt.plot(epochs, g_losses, label=\"Generator Loss\", alpha=0.8)\n",
    "        if c_losses:\n",
    "            plt.plot(epochs, c_losses, label=\"Critic Loss\", alpha=0.8)\n",
    "        plt.title(\"Losses per Epoch\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        if g_losses or c_losses:\n",
    "            plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Plot FID scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if fid_scores and fid_epochs:\n",
    "            plt.plot(fid_epochs, fid_scores, marker='o', linestyle='-', label=\"FID Score\")\n",
    "            if fid_scores:\n",
    "                best_fid_val = min(fid_scores)\n",
    "                best_epoch_idx = fid_scores.index(best_fid_val)\n",
    "                best_epoch = fid_epochs[best_epoch_idx]\n",
    "                plt.scatter([best_epoch], [best_fid_val], color='red', s=100, zorder=5, \n",
    "                            label=f'Best FID: {best_fid_val:.2f} (Epoch {best_epoch})')\n",
    "            plt.legend()\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'FID not calculated or no data.', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        \n",
    "        plt.title(\"FID Score per Check\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"FID Score (Lower is Better)\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(save_dir, \"training_metrics_plot_v2151.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        logger.info(f\"Saved metrics plot to {plot_filename}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate or save plots: {e}\", exc_info=True)\n",
    "\n",
    "# New separate plotting functions\n",
    "def plot_losses(g_losses, c_losses, save_dir):\n",
    "    \"\"\"Plot generator and critic losses separately\"\"\"\n",
    "    try:\n",
    "        if not g_losses or not c_losses:\n",
    "            logger.warning(\"No loss data to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        epochs = range(1, len(g_losses) + 1)\n",
    "        \n",
    "        ax.plot(epochs, g_losses, 'r-', label=\"Generator Loss\", alpha=0.8)\n",
    "        ax.plot(epochs, c_losses, 'b-', label=\"Critic Loss\", alpha=0.8)\n",
    "        \n",
    "        ax.set_title(\"Generator and Critic Loss vs. Epoch\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(save_dir, \"loss_plot_v2151.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        logger.info(f\"Saved loss plot to {plot_filename}\")\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate or save loss plot: {e}\", exc_info=True)\n",
    "\n",
    "def plot_fid(fid_scores, fid_epochs, save_dir):\n",
    "    \"\"\"Plot FID scores with best and worst points annotated\"\"\"\n",
    "    try:\n",
    "        if not fid_scores or not fid_epochs:\n",
    "            logger.warning(\"No FID data to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        ax.plot(fid_epochs, fid_scores, marker='o', linestyle='-', label=\"FID Score\")\n",
    "        \n",
    "        # Find and annotate best (min) FID\n",
    "        best_fid_val = min(fid_scores)\n",
    "        best_epoch_idx = fid_scores.index(best_fid_val)\n",
    "        best_epoch = fid_epochs[best_epoch_idx]\n",
    "        ax.scatter([best_epoch], [best_fid_val], color='red', s=100, zorder=5)\n",
    "        ax.annotate(f'Best: {best_fid_val:.2f}\\nEpoch: {best_epoch}', \n",
    "                    xy=(best_epoch, best_fid_val), xytext=(10, -30),\n",
    "                    textcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))\n",
    "        \n",
    "        # Find and annotate worst (max) FID\n",
    "        worst_fid_val = max(fid_scores)\n",
    "        worst_epoch_idx = fid_scores.index(worst_fid_val)\n",
    "        worst_epoch = fid_epochs[worst_epoch_idx]\n",
    "        ax.scatter([worst_epoch], [worst_fid_val], color='orange', s=100, zorder=5)\n",
    "        ax.annotate(f'Worst: {worst_fid_val:.2f}\\nEpoch: {worst_epoch}', \n",
    "                    xy=(worst_epoch, worst_fid_val), xytext=(-30, 10), \n",
    "                    textcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))\n",
    "        \n",
    "        ax.set_title(\"FID Score vs. Epoch\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"FID Score (Lower is Better)\")\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(save_dir, \"fid_plot_v2151.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        logger.info(f\"Saved FID plot to {plot_filename}\")\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate or save FID plot: {e}\", exc_info=True)\n",
    "\n",
    "def plot_kid(kid_scores, kid_stds, kid_epochs, save_dir):\n",
    "    \"\"\"Plot KID scores with best and worst points annotated\"\"\"\n",
    "    try:\n",
    "        if not kid_scores or not kid_epochs:\n",
    "            logger.warning(\"No KID data to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot with error bars if standard deviations are available\n",
    "        if kid_stds and len(kid_stds) == len(kid_scores):\n",
    "            ax.errorbar(kid_epochs, kid_scores, yerr=kid_stds, fmt='o-', \n",
    "                      label=\"KID Score\", capsize=4)\n",
    "        else:\n",
    "            ax.plot(kid_epochs, kid_scores, marker='o', linestyle='-', label=\"KID Score\")\n",
    "        \n",
    "        # Find and annotate best (min) KID\n",
    "        best_kid_val = min(kid_scores)\n",
    "        best_epoch_idx = kid_scores.index(best_kid_val)\n",
    "        best_epoch = kid_epochs[best_epoch_idx]\n",
    "        ax.scatter([best_epoch], [best_kid_val], color='red', s=100, zorder=5)\n",
    "        ax.annotate(f'Best: {best_kid_val:.4f}\\nEpoch: {best_epoch}', \n",
    "                    xy=(best_epoch, best_kid_val), xytext=(10, -30),\n",
    "                    textcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))\n",
    "        \n",
    "        # Find and annotate worst (max) KID\n",
    "        worst_kid_val = max(kid_scores)\n",
    "        worst_epoch_idx = kid_scores.index(worst_kid_val)\n",
    "        worst_epoch = kid_epochs[worst_epoch_idx]\n",
    "        ax.scatter([worst_epoch], [worst_kid_val], color='orange', s=100, zorder=5)\n",
    "        ax.annotate(f'Worst: {worst_kid_val:.4f}\\nEpoch: {worst_epoch}', \n",
    "                    xy=(worst_epoch, worst_kid_val), xytext=(-30, 10), \n",
    "                    textcoords='offset points', arrowprops=dict(arrowstyle=\"->\"))\n",
    "        \n",
    "        ax.set_title(\"KID Score vs. Epoch\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"KID Score (Lower is Better)\")\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(save_dir, \"kid_plot_v2151.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        logger.info(f\"Saved KID plot to {plot_filename}\")\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate or save KID plot: {e}\", exc_info=True)\n",
    "\n",
    "def plot_combined_metrics(fid_scores, kid_scores, epochs, save_dir):\n",
    "    \"\"\"Plot FID and KID scores on the same graph\"\"\"\n",
    "    try:\n",
    "        if not fid_scores or not kid_scores or not epochs:\n",
    "            logger.warning(\"Missing data for combined metrics plot\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Check if we need different scales for FID and KID\n",
    "        fid_max, fid_min = max(fid_scores), min(fid_scores)\n",
    "        kid_max, kid_min = max(kid_scores), min(kid_scores)\n",
    "        fid_range = fid_max - fid_min\n",
    "        kid_range = kid_max - kid_min\n",
    "        \n",
    "        # If scales are very different, use two y-axes\n",
    "        if fid_range / kid_range > 5 or kid_range / fid_range > 5:\n",
    "            # Plot FID on left axis\n",
    "            line1, = ax.plot(epochs, fid_scores, 'b-o', label=\"FID Score\")\n",
    "            ax.set_ylabel(\"FID Score\", color='b')\n",
    "            ax.tick_params(axis='y', labelcolor='b')\n",
    "            \n",
    "            # Create right y-axis for KID\n",
    "            ax2 = ax.twinx()\n",
    "            line2, = ax2.plot(epochs, kid_scores, 'r-o', label=\"KID Score\")\n",
    "            ax2.set_ylabel(\"KID Score\", color='r')\n",
    "            ax2.tick_params(axis='y', labelcolor='r')\n",
    "            \n",
    "            # Put legends together\n",
    "            lines = [line1, line2]\n",
    "            labels = [line.get_label() for line in lines]\n",
    "            ax.legend(lines, labels, loc=\"upper right\")\n",
    "        else:\n",
    "            # Plot both on same scale\n",
    "            ax.plot(epochs, fid_scores, 'b-o', label=\"FID Score\")\n",
    "            ax.plot(epochs, kid_scores, 'r-o', label=\"KID Score\")\n",
    "            ax.set_ylabel(\"Score Value (Lower is Better)\")\n",
    "            ax.legend()\n",
    "        \n",
    "        ax.set_title(\"FID and KID Scores vs. Epoch\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(save_dir, \"combined_metrics_plot_v2151.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        logger.info(f\"Saved combined metrics plot to {plot_filename}\")\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate or save combined metrics plot: {e}\", exc_info=True)\n",
    "\n",
    "def plot_feature_space(real_features, fake_features, save_path):\n",
    "    \"\"\"Create t-SNE visualization of real vs fake feature distributions\"\"\"\n",
    "    try:\n",
    "        if real_features is None or fake_features is None:\n",
    "            logger.warning(\"Missing features for t-SNE visualization\")\n",
    "            return\n",
    "        \n",
    "        # Sample if too many points\n",
    "        max_samples = TSNE_SAMPLE_SIZE // 2  # Half for real, half for fake\n",
    "        \n",
    "        if len(real_features) > max_samples:\n",
    "            indices = np.random.choice(len(real_features), max_samples, replace=False)\n",
    "            real_sample = real_features[indices]\n",
    "        else:\n",
    "            real_sample = real_features\n",
    "            \n",
    "        if len(fake_features) > max_samples:\n",
    "            indices = np.random.choice(len(fake_features), max_samples, replace=False)\n",
    "            fake_sample = fake_features[indices]\n",
    "        else:\n",
    "            fake_sample = fake_features\n",
    "        \n",
    "        # Combine features for t-SNE\n",
    "        combined_features = np.vstack([real_sample, fake_sample])\n",
    "        \n",
    "        # Create labels (0 for real, 1 for fake)\n",
    "        labels = np.zeros(len(combined_features))\n",
    "        labels[len(real_sample):] = 1\n",
    "        \n",
    "        # Perform t-SNE\n",
    "        logger.info(\"Computing t-SNE embedding...\")\n",
    "        tsne = manifold.TSNE(n_components=2, perplexity=TSNE_PERPLEXITY, \n",
    "                            random_state=TSNE_RANDOM_STATE, n_iter=1000)\n",
    "        embedding = tsne.fit_transform(combined_features)\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        real_points = embedding[:len(real_sample)]\n",
    "        fake_points = embedding[len(real_sample):]\n",
    "        \n",
    "        ax.scatter(real_points[:, 0], real_points[:, 1], c='blue', alpha=0.6, label='Real', s=20)\n",
    "        ax.scatter(fake_points[:, 0], fake_points[:, 1], c='red', alpha=0.6, label='Generated', s=20)\n",
    "        \n",
    "        ax.set_title('t-SNE Visualization of Real vs Generated Feature Distributions')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add a description of what plot shows\n",
    "        ax.annotate(\"Note: Points closer together have similar feature representations\\n\"\n",
    "                    \"Good generation = red points distributed similarly to blue points\", \n",
    "                    xy=(0.5, -0.01), xycoords='axes fraction', \n",
    "                    ha='center', va='top', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        logger.info(f\"Saved t-SNE visualization to {save_path}\")\n",
    "        plt.close(fig)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate or save feature space visualization: {e}\", exc_info=True)\n",
    "\n",
    "# --- Generate Markdown Report ---\n",
    "def generate_markdown_report(g_losses_hist, c_losses_hist, fid_scores_hist, fid_epochs_hist, kid_scores_hist, kid_epochs_hist, training_info):\n",
    "    \"\"\"Generate a simple markdown report of the training results\"\"\"\n",
    "    report_path = os.path.join(ANALYSIS_DIR, \"training_report.md\")\n",
    "    \n",
    "    try:\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(\"# WGAN-SN Training Report (v2.151-modified)\\n\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            # Training summary\n",
    "            f.write(\"## Training Summary\\n\\n\")\n",
    "            f.write(f\"- Total Epochs: {training_info.get('epochs', 'N/A')}\\n\")\n",
    "            f.write(f\"- Final Status: {training_info.get('stop_reason', 'N/A')}\\n\")\n",
    "            f.write(f\"- Total Training Time: {training_info.get('training_time', 0):.2f} seconds\\n\")\n",
    "            f.write(f\"- Final Generator Loss: {g_losses_hist[-1] if g_losses_hist else 'N/A'}\\n\")\n",
    "            f.write(f\"- Final Critic Loss: {c_losses_hist[-1] if c_losses_hist else 'N/A'}\\n\")\n",
    "            \n",
    "            # FID information\n",
    "            if fid_scores_hist:\n",
    "                best_fid = min(fid_scores_hist)\n",
    "                best_fid_epoch = fid_epochs_hist[fid_scores_hist.index(best_fid)]\n",
    "                f.write(f\"- Best FID Score: {best_fid:.4f} (Epoch {best_fid_epoch})\\n\")\n",
    "                f.write(f\"- Final FID Score: {fid_scores_hist[-1]:.4f}\\n\")\n",
    "                \n",
    "            # KID information (new)\n",
    "            if kid_scores_hist:\n",
    "                best_kid = min(kid_scores_hist)\n",
    "                best_kid_epoch = kid_epochs_hist[kid_scores_hist.index(best_kid)]\n",
    "                f.write(f\"- Best KID Score: {best_kid:.4f} (Epoch {best_kid_epoch})\\n\")\n",
    "                f.write(f\"- Final KID Score: {kid_scores_hist[-1]:.4f}\\n\")\n",
    "            \n",
    "            # Loss trends\n",
    "            f.write(\"\\n## Loss Trends\\n\\n\")\n",
    "            if g_losses_hist and len(g_losses_hist) > 10:\n",
    "                recent_g_loss = g_losses_hist[-5:]\n",
    "                early_g_loss = g_losses_hist[:5]\n",
    "                avg_recent = sum(recent_g_loss) / len(recent_g_loss)\n",
    "                avg_early = sum(early_g_loss) / len(early_g_loss)\n",
    "                \n",
    "                f.write(f\"- Initial Generator Loss Avg (first 5 epochs): {sum(early_g_loss)/len(early_g_loss):.4f}\\n\")\n",
    "                f.write(f\"- Final Generator Loss Avg (last 5 epochs): {sum(recent_g_loss)/len(recent_g_loss):.4f}\\n\")\n",
    "                \n",
    "                if avg_recent < -0.5 and avg_recent > -2.0:\n",
    "                    f.write(\"- Generator loss has stabilized in the appropriate negative range typically seen in successful WGAN training.\\n\")\n",
    "                elif avg_recent > -0.5:\n",
    "                    f.write(\"- Generator loss may be too close to zero, which could indicate mode collapse or poor convergence.\\n\")\n",
    "                elif avg_recent < -2.0:\n",
    "                    f.write(\"- Generator loss is very negative, which might indicate training instability.\\n\")\n",
    "                    \n",
    "                if avg_recent > avg_early and g_losses_hist[-1] > g_losses_hist[-10]:\n",
    "                    f.write(\"- Warning: Generator loss is increasing, which suggests potential training instability.\\n\")\n",
    "            \n",
    "            # FID analysis\n",
    "            if fid_scores_hist and len(fid_scores_hist) >= 3:\n",
    "                f.write(\"\\n## FID Analysis\\n\\n\")\n",
    "                last_fids = fid_scores_hist[-3:]\n",
    "                \n",
    "                if last_fids[0] > last_fids[-1] and last_fids[1] > last_fids[-1]:\n",
    "                    f.write(\"- FID scores are continuing to improve, indicating the model is still learning to generate more realistic images.\\n\")\n",
    "                elif all(abs(last_fids[0] - fid) < 1.0 for fid in last_fids[1:]):\n",
    "                    f.write(\"- FID scores have stabilized, suggesting the model has reached convergence.\\n\")\n",
    "                elif last_fids[0] < last_fids[-1] and last_fids[1] < last_fids[-1]:\n",
    "                    f.write(\"- Warning: FID scores are worsening, which could indicate overfitting or training instability.\\n\")\n",
    "            \n",
    "            # KID analysis (new)\n",
    "            if kid_scores_hist and len(kid_scores_hist) >= 3:\n",
    "                f.write(\"\\n## KID Analysis\\n\\n\")\n",
    "                last_kids = kid_scores_hist[-3:]\n",
    "                \n",
    "                if last_kids[0] > last_kids[-1] and last_kids[1] > last_kids[-1]:\n",
    "                    f.write(\"- KID scores are continuing to improve, suggesting better distributional match between real and generated images.\\n\")\n",
    "                elif all(abs(last_kids[0] - kid) < 0.001 for kid in last_kids[1:]):\n",
    "                    f.write(\"- KID scores have stabilized, indicating convergence in the feature distribution matching.\\n\")\n",
    "                elif last_kids[0] < last_kids[-1] and last_kids[1] < last_kids[-1]:\n",
    "                    f.write(\"- Warning: KID scores are worsening, which could indicate mode collapse or overfitting.\\n\")\n",
    "                \n",
    "            # FID vs KID comparison (new)\n",
    "            if fid_scores_hist and kid_scores_hist and len(fid_scores_hist) == len(kid_scores_hist):\n",
    "                f.write(\"\\n## FID vs KID Comparison\\n\\n\")\n",
    "                \n",
    "                # Calculate correlation between FID and KID\n",
    "                try:\n",
    "                    correlation = np.corrcoef(fid_scores_hist, kid_scores_hist)[0, 1]\n",
    "                    f.write(f\"- Correlation between FID and KID scores: {correlation:.4f}\\n\")\n",
    "                    \n",
    "                    if correlation > 0.8:\n",
    "                        f.write(\"- FID and KID are strongly correlated, suggesting they are measuring similar aspects of generation quality.\\n\")\n",
    "                    elif correlation > 0.5:\n",
    "                        f.write(\"- FID and KID show moderate correlation, suggesting they capture somewhat different aspects of generation quality.\\n\")\n",
    "                    else:\n",
    "                        f.write(\"- FID and KID show weak correlation, suggesting they may be measuring different aspects of generation quality.\\n\")\n",
    "                except:\n",
    "                    f.write(\"- Could not calculate correlation between FID and KID.\\n\")\n",
    "                \n",
    "                # Check if best epochs align\n",
    "                best_fid_epoch = fid_epochs_hist[fid_scores_hist.index(min(fid_scores_hist))]\n",
    "                best_kid_epoch = kid_epochs_hist[kid_scores_hist.index(min(kid_scores_hist))]\n",
    "                \n",
    "                if best_fid_epoch == best_kid_epoch:\n",
    "                    f.write(f\"- Best FID and KID scores both occurred at the same epoch ({best_fid_epoch}), strongly validating this as the optimal model.\\n\")\n",
    "                else:\n",
    "                    f.write(f\"- Best FID occurred at epoch {best_fid_epoch}, while best KID occurred at epoch {best_kid_epoch}.\\n\")\n",
    "                    f.write(f\"- This divergence suggests different aspects of quality peaked at different times during training.\\n\")\n",
    "            \n",
    "            # Generated Images\n",
    "            f.write(\"\\n## Generated Images\\n\\n\")\n",
    "            f.write(\"Sample images are saved in the `samples` directory.\\n\")\n",
    "            \n",
    "            # Plots\n",
    "            f.write(\"\\n## Plots\\n\\n\")\n",
    "            f.write(\"- Loss plot: `plots/loss_plot_v2151.png`\\n\")\n",
    "            f.write(\"- FID plot: `plots/fid_plot_v2151.png`\\n\")\n",
    "            f.write(\"- KID plot: `plots/kid_plot_v2151.png`\\n\")\n",
    "            f.write(\"- Combined metrics plot: `plots/combined_metrics_plot_v2151.png`\\n\")\n",
    "            f.write(\"- t-SNE feature space visualization: `plots/feature_space_tsne_v2151.png`\\n\")\n",
    "            \n",
    "            # Recommendations\n",
    "            f.write(\"\\n## Recommendations\\n\\n\")\n",
    "            \n",
    "            if not g_losses_hist:\n",
    "                f.write(\"- No training data available to make recommendations.\\n\")\n",
    "            else:\n",
    "                if fid_scores_hist and min(fid_scores_hist) > 100:\n",
    "                    f.write(\"- FID scores are high. Consider training for more epochs or adjusting hyperparameters.\\n\")\n",
    "                elif fid_scores_hist and min(fid_scores_hist) < 50:\n",
    "                    f.write(\"- FID scores are good. The model is generating realistic images.\\n\")\n",
    "                \n",
    "                if kid_scores_hist and min(kid_scores_hist) > 0.1:\n",
    "                    f.write(\"- KID scores are high. Consider training for more epochs or adjusting training dynamics.\\n\")\n",
    "                elif kid_scores_hist and min(kid_scores_hist) < 0.05:\n",
    "                    f.write(\"- KID scores are good. The model is generating realistic feature distributions.\\n\")\n",
    "                \n",
    "                if training_info.get('epochs', 0) < NUM_EPOCHS and training_info.get('stop_reason', '') not in [\"Early stopping (FID)\", \"Early stopping (KID)\"]:\n",
    "                    f.write(\"- Training was interrupted before completion. Consider resuming training.\\n\")\n",
    "                \n",
    "        logger.info(f\"Generated markdown report at {report_path}\")\n",
    "        return report_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate markdown report: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "# --- Helper functions for KID calculation ---\n",
    "def polynomial_kernel(X, Y):\n",
    "    \"\"\"\n",
    "    Polynomial kernel for KID: k(x,y) = (gamma <x,y> + coef0)^degree\n",
    "    Using carefully balanced parameters to prevent underflow/overflow.\n",
    "    \"\"\"\n",
    "    # Convert to higher precision\n",
    "    X = X.astype(np.float64)\n",
    "    Y = Y.astype(np.float64)\n",
    "    \n",
    "    # Normalize features with slightly relaxed epsilon\n",
    "    X_norm = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-8)\n",
    "    Y_norm = Y / (np.linalg.norm(Y, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    # More balanced parameters that won't underflow\n",
    "    gamma = 0.2\n",
    "    coef0 = 1.0\n",
    "    degree = 3\n",
    "    \n",
    "    dot_product = np.matmul(X_norm, Y_norm.T)\n",
    "    \n",
    "    # Prevent exact zeros with lower bound\n",
    "    return np.clip((gamma * dot_product + coef0) ** degree, 1e-8, 1e6)\n",
    "\n",
    "def calculate_kid_from_features(real_features, fake_features, subset_size=1000, num_subsets=100):\n",
    "    \"\"\"\n",
    "    Calculate KID given features extracted from Inception.\n",
    "    Uses polynomial kernel and subsampling with safeguards against numerical issues.\n",
    "    \"\"\"\n",
    "    # Use high precision\n",
    "    real_features = real_features.astype(np.float64)\n",
    "    fake_features = fake_features.astype(np.float64)\n",
    "    \n",
    "    # Center the features (remove mean) - this is still good practice\n",
    "    real_features = real_features - np.mean(real_features, axis=0, keepdims=True)\n",
    "    fake_features = fake_features - np.mean(fake_features, axis=0, keepdims=True)\n",
    "    \n",
    "    n_r, n_f = real_features.shape[0], fake_features.shape[0]\n",
    "    \n",
    "    subset_size = min(subset_size, min(n_r, n_f))\n",
    "    kid_values = []\n",
    "    \n",
    "    # Verify inputs aren't identical\n",
    "    if np.array_equal(real_features, fake_features):\n",
    "        logger.warning(\"WARNING: real_features and fake_features are identical arrays! KID calculation will be biased.\")\n",
    "    \n",
    "    for _ in range(num_subsets):\n",
    "        # Sample subset_size features from both distributions\n",
    "        r_idx = np.random.choice(n_r, size=subset_size, replace=False)\n",
    "        f_idx = np.random.choice(n_f, size=subset_size, replace=False)\n",
    "        \n",
    "        r_subset = real_features[r_idx]\n",
    "        f_subset = fake_features[f_idx]\n",
    "        \n",
    "        # Calculate polynomial kernel MMD (Maximum Mean Discrepancy)\n",
    "        k_rr = polynomial_kernel(r_subset, r_subset)\n",
    "        k_rf = polynomial_kernel(r_subset, f_subset)\n",
    "        k_ff = polynomial_kernel(f_subset, f_subset)\n",
    "        \n",
    "        # Calculate unbiased MMD estimate with safeguards\n",
    "        n = subset_size\n",
    "        mmd_numerator = np.sum(k_rr) - np.trace(k_rr) + np.sum(k_ff) - np.trace(k_ff) - 2 * np.sum(k_rf)\n",
    "        mmd_denominator = n * (n-1)\n",
    "        \n",
    "        # Prevent division by zero (should never happen with our subset size checks)\n",
    "        if mmd_denominator <= 0:\n",
    "            logger.warning(\"WARNING: Invalid denominator in KID calculation!\")\n",
    "            mmd = 0.01  # Fallback value\n",
    "        else:\n",
    "            mmd = mmd_numerator / mmd_denominator\n",
    "        \n",
    "        # Ensure non-negative MMD and prevent exact zeros\n",
    "        mmd = max(1e-8, mmd)\n",
    "        kid_values.append(mmd)\n",
    "    \n",
    "    return np.mean(kid_values), np.std(kid_values)\n",
    "\n",
    "# --- FID Calculation Utilities ---\n",
    "def get_inception_model(device):\n",
    "    if not FID_AVAILABLE:\n",
    "        raise RuntimeError(\"pytorch-fid library not available.\")\n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "    model = InceptionV3([block_idx]).to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_activations_from_data(dataloader, model, device, num_images, batch_size, desc=\"\"):\n",
    "    if not FID_AVAILABLE:\n",
    "        logger.warning(\"pytorch-fid not available.\")\n",
    "        return None\n",
    "    \n",
    "    n_batches = ceil(num_images / batch_size)\n",
    "    n_used_imgs = 0\n",
    "    pred_list = []\n",
    "    \n",
    "    iterator = iter(dataloader)\n",
    "    logger.info(f\"Calculating activations for {num_images} {desc} images ({n_batches} batches)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_batches), desc=f\"Activations {desc}\", leave=False):\n",
    "            try:\n",
    "                batch = next(iterator).to(device)\n",
    "                if isinstance(batch, (list, tuple)):\n",
    "                    batch = batch[0]\n",
    "                if batch.shape[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                current_batch_size = batch.shape[0]\n",
    "                \n",
    "                if batch.dtype != torch.float32:\n",
    "                    batch = batch.float()\n",
    "                if batch.shape[1] == 1:\n",
    "                    batch = batch.repeat(1, 3, 1, 1)\n",
    "                if batch.shape[1] != 3:\n",
    "                    raise ValueError(f\"Batch needs 3 channels, got {batch.shape[1]}\")\n",
    "                \n",
    "                batch = (batch * 0.5) + 0.5\n",
    "                batch = torch.clamp(batch, 0.0, 1.0)  # Rescale [-1,1] to [0,1]\n",
    "                \n",
    "                pred = model(batch)[0]\n",
    "                if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "                    pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "                \n",
    "                pred_list.append(pred.squeeze(3).squeeze(2).cpu().numpy())\n",
    "                n_used_imgs += current_batch_size\n",
    "                \n",
    "                if n_used_imgs >= num_images:\n",
    "                    break\n",
    "                    \n",
    "                # Clean up to reduce memory usage\n",
    "                del batch, pred\n",
    "                \n",
    "            except StopIteration:\n",
    "                logger.warning(f\"Dataloader exhausted early @ batch {i}. Using {n_used_imgs} images.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during activation batch {i}: {e}\", exc_info=True)\n",
    "                return None\n",
    "    \n",
    "    if not pred_list:\n",
    "        return None\n",
    "    \n",
    "    pred_arr = np.concatenate(pred_list, axis=0)\n",
    "    pred_arr = pred_arr[:num_images]\n",
    "    \n",
    "    return pred_arr\n",
    "\n",
    "def get_generated_activations(generator, inception_model, device, noise_dim, num_images, batch_size, desc=\"\"):\n",
    "    if not FID_AVAILABLE:\n",
    "        logger.warning(\"pytorch-fid not available.\")\n",
    "        return None\n",
    "    \n",
    "    n_batches = ceil(num_images / batch_size)\n",
    "    n_generated_imgs = 0\n",
    "    pred_list = []\n",
    "    \n",
    "    generator.eval()\n",
    "    logger.info(f\"Generating {num_images} fake images & activations ({n_batches} batches)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_batches), desc=f\"Generating & Activating {desc}\", leave=False):\n",
    "            try:\n",
    "                current_batch_size = min(batch_size, num_images - n_generated_imgs)\n",
    "                if current_batch_size <= 0:\n",
    "                    break\n",
    "                \n",
    "                noise = torch.randn(current_batch_size, noise_dim, 1, 1, device=device)\n",
    "                generated_batch = generator(noise)\n",
    "                \n",
    "                if generated_batch.dtype != torch.float32:\n",
    "                    generated_batch = generated_batch.float()\n",
    "                if generated_batch.shape[1] == 1:\n",
    "                    generated_batch = generated_batch.repeat(1, 3, 1, 1)\n",
    "                if generated_batch.shape[1] != 3:\n",
    "                    raise ValueError(f\"Generated batch needs 3 channels, got {generated_batch.shape[1]}\")\n",
    "                \n",
    "                generated_batch = (generated_batch * 0.5) + 0.5\n",
    "                generated_batch = torch.clamp(generated_batch, 0.0, 1.0)  # Rescale\n",
    "                \n",
    "                pred = inception_model(generated_batch)[0]\n",
    "                if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "                    pred = F.adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "                \n",
    "                pred_list.append(pred.squeeze(3).squeeze(2).cpu().numpy())\n",
    "                n_generated_imgs += current_batch_size\n",
    "                \n",
    "                # Clean up to reduce memory usage\n",
    "                del noise, generated_batch, pred\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during generated activation batch {i}: {e}\", exc_info=True)\n",
    "                generator.train()\n",
    "                return None\n",
    "    \n",
    "    generator.train()\n",
    "    \n",
    "    if not pred_list:\n",
    "        return None\n",
    "    \n",
    "    pred_arr = np.concatenate(pred_list, axis=0)\n",
    "    pred_arr = pred_arr[:num_images]\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return pred_arr\n",
    "\n",
    "def get_real_stats(real_dataloader, real_stats_path, real_features_path, inception_model, device, num_images, batch_size, force_recalculate=False):\n",
    "    \"\"\"Updated to also save raw feature array for KID calculation and t-SNE visualization\"\"\"\n",
    "    if os.path.exists(real_stats_path) and os.path.exists(real_features_path) and not force_recalculate:\n",
    "        logger.info(f\"Loading pre-calculated real stats and features from: {real_stats_path} and {real_features_path}\")\n",
    "        try:\n",
    "            stats = np.load(real_stats_path)\n",
    "            mu_real, sigma_real = stats['mu'], stats['sigma']\n",
    "            \n",
    "            # Load real features (for KID calculation and visualization)\n",
    "            real_features = np.load(real_features_path)\n",
    "            \n",
    "            if (mu_real is not None and sigma_real is not None and \n",
    "                mu_real.shape == (2048,) and sigma_real.shape == (2048, 2048) and\n",
    "                real_features.shape[1] == 2048):\n",
    "                 logger.info(\"Loaded real stats and features successfully.\")\n",
    "                 return mu_real, sigma_real, real_features\n",
    "            else:\n",
    "                logger.warning(\"Loaded real stats or features invalid. Recalculating...\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load real stats or features file ({e}). Recalculating...\")\n",
    "    \n",
    "    logger.info(f\"Calculating FID stats and features for {num_images} real images...\")\n",
    "    real_activations = get_activations_from_data(real_dataloader, inception_model, device, num_images, batch_size, desc=\"Real\")\n",
    "    \n",
    "    if real_activations is None or len(real_activations) < num_images:\n",
    "        logger.error(f\"Failed to get enough real activations. Cannot calculate stats.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    mu_real = np.mean(real_activations, axis=0)\n",
    "    sigma_real = np.cov(real_activations, rowvar=False)\n",
    "    \n",
    "    logger.info(f\"Calculated real stats (mu: {mu_real.shape}, sigma: {sigma_real.shape}).\")\n",
    "    \n",
    "    try:\n",
    "        # Save stats for FID\n",
    "        os.makedirs(os.path.dirname(real_stats_path), exist_ok=True)\n",
    "        np.savez(real_stats_path, mu=mu_real, sigma=sigma_real)\n",
    "        logger.info(f\"Saved real FID stats to: {real_stats_path}\")\n",
    "        \n",
    "        # Save raw features for KID and visualization\n",
    "        np.save(real_features_path, real_activations)\n",
    "        logger.info(f\"Saved real features to: {real_features_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save real stats or features: {e}\", exc_info=True)\n",
    "    \n",
    "    return mu_real, sigma_real, real_activations\n",
    "\n",
    "def calculate_fid_and_kid(generator, inception_model, real_mu, real_sigma, real_features, device, noise_dim, num_images, batch_size):\n",
    "    \"\"\"Calculate both FID and KID metrics using the same generated features\"\"\"\n",
    "    if not FID_AVAILABLE:\n",
    "        logger.warning(\"pytorch-fid not available.\")\n",
    "        return float('inf'), (float('inf'), float('inf')), None\n",
    "    \n",
    "    if real_mu is None or real_sigma is None or real_features is None:\n",
    "        logger.error(\"Real stats or features not available.\")\n",
    "        return float('inf'), (float('inf'), float('inf')), None\n",
    "    \n",
    "    logger.info(f\"Calculating FID and KID using {num_images} generated images...\")\n",
    "    fake_features = get_generated_activations(generator, inception_model, device, noise_dim, num_images, batch_size, desc=\"Fake (FID/KID)\")\n",
    "    \n",
    "    if fake_features is None or len(fake_features) < num_images:\n",
    "        logger.error(f\"Failed to get enough fake activations.\")\n",
    "        return float('inf'), (float('inf'), float('inf')), None\n",
    "    \n",
    "    # Calculate FID\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    sigma_fake = np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    logger.info(\"Calculating Frechet distance...\")\n",
    "    \n",
    "    try:\n",
    "        fid_value = calculate_frechet_distance(mu_fake, sigma_fake, real_mu, real_sigma)\n",
    "        logger.info(f\"Calculated FID: {fid_value:.4f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating Frechet distance: {e}\", exc_info=True)\n",
    "        fid_value = float('inf')\n",
    "    \n",
    "    # Calculate KID\n",
    "    logger.info(f\"Calculating KID with {KID_SUBSET_SIZE} samples, {KID_SUBSETS} subsets...\")\n",
    "    try:\n",
    "        # Check if we're using library function or our own implementation\n",
    "        if 'calculate_kid_given_features' in globals():\n",
    "            kid_mean, kid_std = calculate_kid_given_features(real_features, fake_features, \n",
    "                                                             subset_size=KID_SUBSET_SIZE,\n",
    "                                                             num_subsets=KID_SUBSETS)\n",
    "        else:\n",
    "            kid_mean, kid_std = calculate_kid_from_features(real_features, fake_features, \n",
    "                                                           subset_size=KID_SUBSET_SIZE,\n",
    "                                                           num_subsets=KID_SUBSETS)\n",
    "        logger.info(f\"Calculated KID: {kid_mean:.6f}  {kid_std:.6f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating KID: {e}\", exc_info=True)\n",
    "        kid_mean, kid_std = float('inf'), float('inf')\n",
    "    \n",
    "    # Clean up\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return fid_value, (kid_mean, kid_std), fake_features\n",
    "\n",
    "# Function to generate all visualizations after training\n",
    "def generate_all_visualizations(g_losses_hist, c_losses_hist, fid_scores_hist, fid_epochs_hist, kid_scores_hist, kid_std_hist, kid_epochs_hist, real_features=None, fake_features=None, save_dir=PLOT_DIR):\n",
    "    \"\"\"Generate all visualization plots in separate files\"\"\"\n",
    "    logger.info(\"Generating all visualization plots...\")\n",
    "    \n",
    "    # Generate individual plots\n",
    "    plot_losses(g_losses_hist, c_losses_hist, save_dir)\n",
    "    plot_fid(fid_scores_hist, fid_epochs_hist, save_dir)\n",
    "    \n",
    "    if kid_scores_hist and kid_epochs_hist:\n",
    "        plot_kid(kid_scores_hist, kid_std_hist, kid_epochs_hist, save_dir)\n",
    "        \n",
    "        # Generate combined FID/KID plot if both are available\n",
    "        if fid_scores_hist and fid_epochs_hist and len(fid_scores_hist) == len(kid_scores_hist):\n",
    "            plot_combined_metrics(fid_scores_hist, kid_scores_hist, fid_epochs_hist, save_dir)\n",
    "    \n",
    "    # Generate feature space visualization if features are available\n",
    "    if VISUALIZE_TSNE and real_features is not None and fake_features is not None:\n",
    "        tsne_path = os.path.join(save_dir, \"feature_space_tsne_v2151.png\")\n",
    "        plot_feature_space(real_features, fake_features, tsne_path)\n",
    "    \n",
    "    # Original plot_metrics call is commented out\n",
    "    # plot_metrics(g_losses_hist, c_losses_hist, fid_scores_hist, fid_epochs_hist, save_dir)\n",
    "    \n",
    "    logger.info(\"All visualization plots generated.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Main Training Execution ---\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    # Wrap execution in try/finally for cleanup\n",
    "    training_successful = False\n",
    "    stop_reason = \"Unknown\"\n",
    "    g_losses_hist = []\n",
    "    c_losses_hist = []\n",
    "    fid_scores_hist = []\n",
    "    fid_epochs_hist = []\n",
    "    kid_scores_hist = []  \n",
    "    kid_std_hist = []     \n",
    "    kid_epochs_hist = []  \n",
    "    best_fid = float('inf')\n",
    "    best_kid = float('inf')  \n",
    "    epochs_no_improve = 0\n",
    "    final_real_features = None  # For feature space visualization\n",
    "    best_fake_features = None  # For feature space visualization from best model\n",
    "    \n",
    "    try: \n",
    "        # --- Log parameters ---\n",
    "        logger.info(\"--- Starting WGAN-SN Training with enhancements (v2.151-modified) ---\") \n",
    "        config = {k: v for k, v in globals().items() if k.isupper() and not k.startswith('_')}\n",
    "        logger.info(\"Configuration:\\n\" + json.dumps(config, indent=4, default=str))\n",
    "        \n",
    "        # Save configuration\n",
    "        config_save_path = os.path.join(OUTPUT_DIR, \"training_config_v2151.json\")\n",
    "        try:\n",
    "            with open(config_save_path, 'w') as f:\n",
    "                json.dump(config, f, indent=4, default=str)\n",
    "            logger.info(f\"Saved configuration to {config_save_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save configuration: {e}\")\n",
    "\n",
    "        # --- Setup Device ---\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        \n",
    "        if device.type == \"cuda\":\n",
    "            logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            logger.info(f\"GPU Name: {gpu_name}\")\n",
    "        \n",
    "        # Disable AMP as requested\n",
    "        amp_enabled = False\n",
    "        logger.info(f\"AMP (Automatic Mixed Precision) enabled: {amp_enabled}\")\n",
    "        \n",
    "        # Track initial memory\n",
    "        log_gpu_memory_usage(\"Initialization\")\n",
    "\n",
    "        # --- Validate directories before continuing ---\n",
    "        if not validate_directory(PREPROCESSED_DATA_DIR):\n",
    "            logger.critical(f\"Input data directory not found: {PREPROCESSED_DATA_DIR}\")\n",
    "            raise FileNotFoundError(f\"Input data directory not found: {PREPROCESSED_DATA_DIR}\")\n",
    "\n",
    "        # --- Setup Dataset and DataLoader ---\n",
    "        logger.info(\"Setting up Dataset and DataLoader...\")\n",
    "        # Define transformations, including normalization to [-1, 1] and augmentations\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            # Instantiate imported dataset class\n",
    "            dataset = PollenDataset(\n",
    "                root_dir=PREPROCESSED_DATA_DIR, \n",
    "                transform=transform,\n",
    "                image_size=IMAGE_SIZE,      \n",
    "                channels_img=CHANNELS_IMG \n",
    "            )\n",
    "            if len(dataset) == 0:\n",
    "                raise ValueError(\"Dataset is empty.\")\n",
    "            \n",
    "            # Optimize worker count based on CPU cores\n",
    "            try:\n",
    "                dataloader_num_workers = min(max(os.cpu_count() // 2, 1), 4)  # Cap at 4\n",
    "            except:\n",
    "                dataloader_num_workers = 2  # Default if can't determine\n",
    "                \n",
    "            dataloader_pin_memory = (device.type == 'cuda')\n",
    "            dataloader_persistent_workers = False  # Disabled as requested for memory efficiency\n",
    "            \n",
    "            logger.info(f\"DataLoader using num_workers={dataloader_num_workers}, pin_memory={dataloader_pin_memory}, persistent_workers={dataloader_persistent_workers}.\")\n",
    "            \n",
    "            dataloader = DataLoader(\n",
    "                dataset, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                shuffle=True, \n",
    "                num_workers=dataloader_num_workers, \n",
    "                pin_memory=dataloader_pin_memory, \n",
    "                persistent_workers=dataloader_persistent_workers,\n",
    "                drop_last=True\n",
    "            ) \n",
    "            \n",
    "            logger.info(f\"DataLoader created with {len(dataloader)} batches per epoch.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create Dataset/DataLoader: {e}\", exc_info=True)\n",
    "            raise  # Let the outer try/except handle the error\n",
    "\n",
    "        # --- Initialize Models and Optimizers ---\n",
    "        logger.info(\"Initializing models and optimizers...\")\n",
    "        generator = Generator(NOISE_DIM, CHANNELS_IMG, G_FEATURES).to(device)\n",
    "        critic = CriticSN(CHANNELS_IMG, C_FEATURES).to(device) \n",
    "        initialize_weights(generator)\n",
    "        initialize_weights(critic)\n",
    "        logger.info(\"Models initialized with specified weights.\")\n",
    "\n",
    "        opt_gen = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "        opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "        # Initialize GradScalers using updated API\n",
    "        scaler_critic = GradScaler(enabled=amp_enabled)\n",
    "        scaler_gen = GradScaler(enabled=amp_enabled)\n",
    "\n",
    "        # --- Prepare for FID/KID Calculation if enabled ---\n",
    "        inception_model = None\n",
    "        real_mu = None\n",
    "        real_sigma = None\n",
    "        real_features = None\n",
    "        \n",
    "        if (CALCULATE_FID or CALCULATE_KID) and FID_AVAILABLE:\n",
    "            logger.info(\"Preparing for FID/KID calculation...\")\n",
    "            try:\n",
    "                inception_model = get_inception_model(device)\n",
    "                logger.info(\"InceptionV3 model loaded for FID/KID.\")\n",
    "                \n",
    "                fid_transform = transforms.Compose([\n",
    "                    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5] * CHANNELS_IMG, [0.5] * CHANNELS_IMG)\n",
    "                ])\n",
    "                \n",
    "                fid_dataset = PollenDataset(\n",
    "                    PREPROCESSED_DATA_DIR, \n",
    "                    transform=fid_transform, \n",
    "                    image_size=IMAGE_SIZE, \n",
    "                    channels_img=CHANNELS_IMG\n",
    "                )\n",
    "                \n",
    "                actual_fid_num_images = min(FID_NUM_IMAGES, len(fid_dataset))\n",
    "                if actual_fid_num_images < FID_NUM_IMAGES:\n",
    "                    logger.warning(f\"Using {actual_fid_num_images} images for FID/KID based on dataset size.\")\n",
    "                \n",
    "                fid_dataloader = DataLoader(\n",
    "                    fid_dataset, \n",
    "                    batch_size=FID_BATCH_SIZE, \n",
    "                    shuffle=False, \n",
    "                    num_workers=dataloader_num_workers, \n",
    "                    pin_memory=dataloader_pin_memory\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"Created DataLoader for real image FID/KID stats ({len(fid_dataloader)} batches).\")\n",
    "                \n",
    "                real_mu, real_sigma, real_features = get_real_stats(\n",
    "                    fid_dataloader, \n",
    "                    REAL_STATS_PATH,\n",
    "                    REAL_FEATURES_PATH,\n",
    "                    inception_model, \n",
    "                    device, \n",
    "                    actual_fid_num_images, \n",
    "                    FID_BATCH_SIZE, \n",
    "                    FORCE_RECALCULATE_REAL_STATS\n",
    "                )\n",
    "                \n",
    "                if real_mu is None or real_sigma is None or real_features is None:\n",
    "                    logger.error(\"Failed to get real image FID/KID statistics. Disabling FID/KID calculation.\")\n",
    "                    CALCULATE_FID = False\n",
    "                    CALCULATE_KID = False\n",
    "                else:\n",
    "                    logger.info(\"Successfully obtained real image statistics and features.\")\n",
    "                    # Save features for visualization\n",
    "                    final_real_features = real_features\n",
    "                \n",
    "                # Cleanup for memory efficiency\n",
    "                del fid_dataloader, fid_dataset, fid_transform\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as fid_setup_e:\n",
    "                logger.error(f\"Error during FID/KID setup: {fid_setup_e}\", exc_info=True)\n",
    "                logger.error(\"Disabling FID/KID calculation.\")\n",
    "                CALCULATE_FID = False\n",
    "                CALCULATE_KID = False\n",
    "                inception_model = None\n",
    "\n",
    "        # --- Load Checkpoint if Resuming ---\n",
    "        start_epoch = 0\n",
    "        global_step = 0\n",
    "        \n",
    "        if RESUME_TRAINING:\n",
    "            start_epoch, global_step, g_losses_hist, c_losses_hist, fid_scores_hist, fid_epochs_hist, kid_scores_hist, kid_std_hist, kid_epochs_hist, best_fid, best_kid, epochs_no_improve = load_checkpoint(\n",
    "                CHECKPOINT_FILE, generator, critic, opt_gen, opt_critic, scaler_gen, scaler_critic\n",
    "            ) \n",
    "            \n",
    "            # Ensure models and optimizer states are on the correct device after loading\n",
    "            generator.to(device) \n",
    "            critic.to(device)\n",
    "            \n",
    "            for state in opt_gen.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        state[k] = v.to(device)\n",
    "                        \n",
    "            for state in opt_critic.state.values():\n",
    "                for k, v in state.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        state[k] = v.to(device)\n",
    "\n",
    "        # --- Prepare for Training ---\n",
    "        fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device) # Fixed noise for consistent samples\n",
    "\n",
    "        # Set models to training mode\n",
    "        generator.train()\n",
    "        critic.train()\n",
    "        logger.info(f\"--- Starting Training Loop from Epoch {start_epoch+1}, Step {global_step} ---\")\n",
    "\n",
    "        # Check memory usage before training\n",
    "        log_gpu_memory_usage(\"Before Training Loop\")\n",
    "\n",
    "        # ==================== TRAINING LOOP ====================\n",
    "        early_stop_triggered = False\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "            epoch_start_time = time.time()\n",
    "            # Progress bar for batches within the epoch\n",
    "            loop_pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=True, desc=f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\") \n",
    "            \n",
    "            # Track average losses for the epoch\n",
    "            avg_loss_c_epoch = 0.0\n",
    "            avg_loss_g_epoch = 0.0\n",
    "            batches_in_epoch = 0\n",
    "\n",
    "            for batch_idx, real_images in loop_pbar:\n",
    "                \n",
    "                # --- Optional: GPU Temperature Check ---\n",
    "                if MONITOR_TEMP and NVML_AVAILABLE and (global_step % 200 == 0): \n",
    "                     gpu_temp = get_gpu_temp(GPU_ID)\n",
    "                     if gpu_temp is not None:\n",
    "                          logger.info(f\"Step {global_step} | GPU Temp: {gpu_temp}C\")\n",
    "                          if gpu_temp > GPU_TEMP_THRESHOLD:\n",
    "                               logger.warning(f\"GPU Temperature {gpu_temp}C exceeded threshold {GPU_TEMP_THRESHOLD}C!\")\n",
    "                               logger.warning(\"Saving checkpoint and stopping training gracefully.\")\n",
    "                               save_checkpoint({\n",
    "                                   'epoch': epoch, \n",
    "                                   'step': global_step, \n",
    "                                   'generator_state_dict': generator.state_dict(),\n",
    "                                   'critic_state_dict': critic.state_dict(),\n",
    "                                   'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                                   'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                                   'scaler_gen_state_dict': scaler_gen.state_dict(),\n",
    "                                   'scaler_critic_state_dict': scaler_critic.state_dict(),\n",
    "                                   'g_losses_history': g_losses_hist,\n",
    "                                   'c_losses_history': c_losses_hist,\n",
    "                                   'fid_scores_history': fid_scores_hist,\n",
    "                                   'fid_epochs_history': fid_epochs_hist,\n",
    "                                   'kid_scores_history': kid_scores_hist,\n",
    "                                   'kid_std_history': kid_std_hist,\n",
    "                                   'kid_epochs_history': kid_epochs_hist,\n",
    "                                   'best_fid': best_fid,\n",
    "                                   'best_kid': best_kid,\n",
    "                                   'epochs_no_improve': epochs_no_improve\n",
    "                               }, CHECKPOINT_FILE)\n",
    "                               stop_reason = f\"GPU Temp {gpu_temp}C > Threshold {GPU_TEMP_THRESHOLD}C\"\n",
    "                               early_stop_triggered = True\n",
    "                               break\n",
    "\n",
    "                # --- Main Training Step ---\n",
    "                try:\n",
    "                    if real_images is None: \n",
    "                         logger.warning(f\"Skipping batch {batch_idx} due to None data.\")\n",
    "                         continue\n",
    "                         \n",
    "                    real_images = real_images.to(device)\n",
    "                    cur_batch_size = real_images.shape[0]\n",
    "                    if cur_batch_size == 0:\n",
    "                        continue \n",
    "\n",
    "                    # --- Train Critic ---\n",
    "                    critic_loss_accum_iter = 0.0\n",
    "                    opt_critic.zero_grad(set_to_none=True) \n",
    "                    \n",
    "                    for _ in range(CRITIC_ITERATIONS): \n",
    "                        noise = torch.randn(cur_batch_size, NOISE_DIM, 1, 1).to(device)\n",
    "                        \n",
    "                        with autocast(device_type='cuda', enabled=amp_enabled):\n",
    "                             with torch.no_grad():\n",
    "                                 fake_images = generator(noise) \n",
    "                             critic_real = critic(real_images).reshape(-1)\n",
    "                             critic_fake = critic(fake_images).reshape(-1) \n",
    "                             loss_critic = torch.mean(critic_fake) - torch.mean(critic_real)\n",
    "                             \n",
    "                        critic_loss_accum_iter += loss_critic.item()\n",
    "                        \n",
    "                        # Scale the loss for backprop\n",
    "                        scaler_critic.scale(loss_critic).backward()\n",
    "                    \n",
    "                    # Update critic with gradient clipping\n",
    "                    scaler_critic.unscale_(opt_critic)\n",
    "                    torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)\n",
    "                    scaler_critic.step(opt_critic)\n",
    "                    scaler_critic.update()\n",
    "                    \n",
    "                    avg_loss_c_iter = critic_loss_accum_iter / CRITIC_ITERATIONS \n",
    "\n",
    "                    # --- Train Generator --- \n",
    "                    opt_gen.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    with autocast(device_type='cuda', enabled=amp_enabled):\n",
    "                         noise_for_g = torch.randn(cur_batch_size, NOISE_DIM, 1, 1).to(device)\n",
    "                         fake_images_for_g = generator(noise_for_g)\n",
    "                         critic_fake_for_gen = critic(fake_images_for_g).reshape(-1) \n",
    "                         loss_gen = -torch.mean(critic_fake_for_gen)\n",
    "                    \n",
    "                    # Scale, backward, clip, and step\n",
    "                    scaler_gen.scale(loss_gen).backward()\n",
    "                    scaler_gen.unscale_(opt_gen)\n",
    "                    torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "                    scaler_gen.step(opt_gen)\n",
    "                    scaler_gen.update()\n",
    "\n",
    "                    # --- Logging and Visualization ---\n",
    "                    loss_g_item = loss_gen.item() \n",
    "                    avg_loss_c_epoch += avg_loss_c_iter\n",
    "                    avg_loss_g_epoch += loss_g_item\n",
    "                    batches_in_epoch += 1\n",
    "\n",
    "                    # Periodically log losses\n",
    "                    if global_step % 100 == 0: \n",
    "                         logger.debug(f\"Step {global_step} | Loss C: {avg_loss_c_iter:.4f}, Loss G: {loss_g_item:.4f}\")\n",
    "                         # Track memory usage\n",
    "                         log_gpu_memory_usage(f\"Step {global_step}\")\n",
    "\n",
    "                    # Save sample images periodically based on global step\n",
    "                    if global_step > 0 and global_step % SAMPLE_FREQ_STEPS == 0:\n",
    "                        logger.info(f\"Saving samples at step {global_step}\")\n",
    "                        generator.eval()\n",
    "                        critic.eval() \n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            with autocast(device_type='cuda', enabled=amp_enabled): \n",
    "                                fake_samples = generator(fixed_noise) \n",
    "                            img_grid = vutils.make_grid(fake_samples * 0.5 + 0.5, normalize=False) \n",
    "                            vutils.save_image(img_grid, os.path.join(SAMPLE_DIR, f\"sample_{epoch+1:04d}_{global_step:07d}.png\"))\n",
    "                        \n",
    "                        generator.train()\n",
    "                        critic.train() \n",
    "                        \n",
    "                        # Clean up sample generation tensors\n",
    "                        del fake_samples, img_grid\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                    # Update progress bar description\n",
    "                    loop_pbar.set_description(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\") \n",
    "                    loop_pbar.set_postfix(loss_C=avg_loss_c_iter, loss_G=loss_g_item, step=global_step)\n",
    "                    \n",
    "                    # Clean up tensors to reduce memory usage\n",
    "                    del real_images, noise, fake_images, critic_real, critic_fake, loss_critic\n",
    "                    del noise_for_g, fake_images_for_g, critic_fake_for_gen, loss_gen\n",
    "                    \n",
    "                    global_step += 1\n",
    "\n",
    "                # --- Error Handling for Batch ---\n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Runtime error processing batch {batch_idx} at Step {global_step}: {e}\", exc_info=True) \n",
    "                    if \"out of memory\" in str(e).lower():\n",
    "                        logger.error(f\"CUDA Out Of Memory! Batch Size: {BATCH_SIZE}. Consider reducing batch size.\")\n",
    "                        logger.warning(\"Attempting to save checkpoint before stopping...\")\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch, \n",
    "                            'step': global_step, \n",
    "                            'generator_state_dict': generator.state_dict(),\n",
    "                            'critic_state_dict': critic.state_dict(),\n",
    "                            'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                            'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                            'scaler_gen_state_dict': scaler_gen.state_dict(),\n",
    "                            'scaler_critic_state_dict': scaler_critic.state_dict(),\n",
    "                            'g_losses_history': g_losses_hist,\n",
    "                            'c_losses_history': c_losses_hist,\n",
    "                            'fid_scores_history': fid_scores_hist,\n",
    "                            'fid_epochs_history': fid_epochs_hist,\n",
    "                            'kid_scores_history': kid_scores_hist,\n",
    "                            'kid_std_history': kid_std_hist,\n",
    "                            'kid_epochs_history': kid_epochs_hist,\n",
    "                            'best_fid': best_fid,\n",
    "                            'best_kid': best_kid,\n",
    "                            'epochs_no_improve': epochs_no_improve\n",
    "                        }, CHECKPOINT_FILE)\n",
    "                        stop_reason = \"CUDA Out of Memory\"\n",
    "                        raise e \n",
    "                    raise e \n",
    "                    \n",
    "                except Exception as e: \n",
    "                    logger.error(f\"Generic error processing batch {batch_idx} at step {global_step}: {e}\", exc_info=True)\n",
    "                    raise e\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if early_stop_triggered:\n",
    "                break\n",
    "                \n",
    "            # --- End of Batch Loop ---\n",
    "\n",
    "            # --- End of Epoch ---\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            if batches_in_epoch > 0:\n",
    "                 avg_loss_c_epoch /= batches_in_epoch\n",
    "                 avg_loss_g_epoch /= batches_in_epoch\n",
    "                 \n",
    "            # Append losses to history\n",
    "            g_losses_hist.append(avg_loss_g_epoch)\n",
    "            c_losses_hist.append(avg_loss_c_epoch)\n",
    "                 \n",
    "            logger.info(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Completed in {epoch_duration:.2f}s | Avg Loss C: {avg_loss_c_epoch:.4f} | Avg Loss G: {avg_loss_g_epoch:.4f}\")\n",
    "\n",
    "            # --- Memory cleanup ---\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            log_gpu_memory_usage(f\"After Epoch {epoch+1}\")\n",
    "            \n",
    "            # --- FID/KID Calculation ---\n",
    "            if (CALCULATE_FID or CALCULATE_KID) and FID_AVAILABLE and inception_model is not None:\n",
    "                current_fid, (current_kid, current_kid_std), fake_features_epoch = calculate_fid_and_kid(\n",
    "                    generator, \n",
    "                    inception_model, \n",
    "                    real_mu, \n",
    "                    real_sigma,\n",
    "                    real_features,\n",
    "                    device, \n",
    "                    NOISE_DIM, \n",
    "                    FID_NUM_IMAGES, \n",
    "                    FID_BATCH_SIZE\n",
    "                )\n",
    "                \n",
    "                # Track FID if calculated and valid\n",
    "                if CALCULATE_FID and current_fid != float('inf'):\n",
    "                    fid_scores_hist.append(current_fid)\n",
    "                    fid_epochs_hist.append(epoch + 1)\n",
    "                    logger.info(f\"--- FID Score @ Epoch {epoch+1}: {current_fid:.4f} ---\")\n",
    "                \n",
    "                # Track KID if calculated and valid\n",
    "                if CALCULATE_KID and current_kid != float('inf'):\n",
    "                    kid_scores_hist.append(current_kid)\n",
    "                    kid_std_hist.append(current_kid_std)\n",
    "                    kid_epochs_hist.append(epoch + 1)\n",
    "                    logger.info(f\"--- KID Score @ Epoch {epoch+1}: {current_kid:.6f}  {current_kid_std:.6f} ---\")\n",
    "                \n",
    "                # Early stopping check\n",
    "                if USE_EARLY_STOPPING:\n",
    "                    metric_improved = False\n",
    "                    \n",
    "                    # Check if the primary chosen metric improved\n",
    "                    if PRIMARY_EVAL_METRIC == \"FID\" and CALCULATE_FID and current_fid < best_fid:\n",
    "                        logger.info(f\"FID improved: {best_fid:.4f} -> {current_fid:.4f}. Saving best FID checkpoint.\")\n",
    "                        best_fid = current_fid\n",
    "                        metric_improved = True\n",
    "                        \n",
    "                        # Save best FID checkpoint (using fixed name to overwrite previous best)\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1, \n",
    "                            'step': global_step, \n",
    "                            'generator_state_dict': generator.state_dict(),\n",
    "                            'critic_state_dict': critic.state_dict(),\n",
    "                            'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                            'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                            'scaler_gen_state_dict': scaler_gen.state_dict(),\n",
    "                            'scaler_critic_state_dict': scaler_critic.state_dict(),\n",
    "                            'g_losses_history': g_losses_hist,\n",
    "                            'c_losses_history': c_losses_hist,\n",
    "                            'fid_scores_history': fid_scores_hist,\n",
    "                            'fid_epochs_history': fid_epochs_hist,\n",
    "                            'kid_scores_history': kid_scores_hist,\n",
    "                            'kid_std_history': kid_std_hist,\n",
    "                            'kid_epochs_history': kid_epochs_hist,\n",
    "                            'best_fid': best_fid,\n",
    "                            'best_kid': best_kid,\n",
    "                            'epochs_no_improve': epochs_no_improve\n",
    "                        }, BEST_FID_CHECKPOINT_FILE)\n",
    "                        \n",
    "                        # Save the best features for visualization\n",
    "                        if fake_features_epoch is not None:\n",
    "                            best_fake_features = fake_features_epoch\n",
    "                            save_fake_features(fake_features_epoch)\n",
    "                        \n",
    "                    elif PRIMARY_EVAL_METRIC == \"KID\" and CALCULATE_KID and current_kid < best_kid:\n",
    "                        logger.info(f\"KID improved: {best_kid:.6f} -> {current_kid:.6f}. Saving best KID checkpoint.\")\n",
    "                        best_kid = current_kid\n",
    "                        metric_improved = True\n",
    "                        \n",
    "                        # Save best KID checkpoint (using fixed name to overwrite previous best)\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1, \n",
    "                            'step': global_step, \n",
    "                            'generator_state_dict': generator.state_dict(),\n",
    "                            'critic_state_dict': critic.state_dict(),\n",
    "                            'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                            'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                            'scaler_gen_state_dict': scaler_gen.state_dict(),\n",
    "                            'scaler_critic_state_dict': scaler_critic.state_dict(),\n",
    "                            'g_losses_history': g_losses_hist,\n",
    "                            'c_losses_history': c_losses_hist,\n",
    "                            'fid_scores_history': fid_scores_hist,\n",
    "                            'fid_epochs_history': fid_epochs_hist,\n",
    "                            'kid_scores_history': kid_scores_hist,\n",
    "                            'kid_std_history': kid_std_hist,\n",
    "                            'kid_epochs_history': kid_epochs_hist,\n",
    "                            'best_fid': best_fid,\n",
    "                            'best_kid': best_kid,\n",
    "                            'epochs_no_improve': epochs_no_improve\n",
    "                        }, BEST_KID_CHECKPOINT_FILE)\n",
    "                        \n",
    "                        # Save the best features for visualization\n",
    "                        if fake_features_epoch is not None:\n",
    "                            best_fake_features = fake_features_epoch\n",
    "                            save_fake_features(fake_features_epoch)\n",
    "                    \n",
    "                    # Track non-primary metric improvements too, but don't consider for early stopping\n",
    "                    if PRIMARY_EVAL_METRIC == \"KID\" and CALCULATE_FID and current_fid < best_fid:\n",
    "                        logger.info(f\"FID improved: {best_fid:.4f} -> {current_fid:.4f}. (Not primary metric)\")\n",
    "                        best_fid = current_fid\n",
    "                        \n",
    "                        # Save best FID checkpoint (using fixed name to overwrite previous best)\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1, \n",
    "                            'step': global_step, \n",
    "                            'generator_state_dict': generator.state_dict(),\n",
    "                            'critic_state_dict': critic.state_dict(),\n",
    "                            'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                            'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                            'scaler_gen_state_dict': scaler_gen.state_dict(),\n",
    "                            'scaler_critic_state_dict': scaler_critic.state_dict(),\n",
    "                            'g_losses_history': g_losses_hist,\n",
    "                            'c_losses_history': c_losses_hist,\n",
    "                            'fid_scores_history': fid_scores_hist,\n",
    "                            'fid_epochs_history': fid_epochs_hist,\n",
    "                            'kid_scores_history': kid_scores_hist,\n",
    "                            'kid_std_history': kid_std_hist,\n",
    "                            'kid_epochs_history': kid_epochs_hist,\n",
    "                            'best_fid': best_fid,\n",
    "                            'best_kid': best_kid,\n",
    "                            'epochs_no_improve': epochs_no_improve\n",
    "                        }, BEST_FID_CHECKPOINT_FILE)\n",
    "                        \n",
    "                    elif PRIMARY_EVAL_METRIC == \"FID\" and CALCULATE_KID and current_kid < best_kid:\n",
    "                        logger.info(f\"KID improved: {best_kid:.6f} -> {current_kid:.6f}. (Not primary metric)\")\n",
    "                        best_kid = current_kid\n",
    "                        \n",
    "                        # Save best KID checkpoint (using fixed name to overwrite previous best)\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1, \n",
    "                            'step': global_step, \n",
    "                            'generator_state_dict': generator.state_dict(),\n",
    "                            'critic_state_dict': critic.state_dict(),\n",
    "                            'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                            'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                            'scaler_gen_state_dict': scaler_gen.state_dict(),\n",
    "                            'scaler_critic_state_dict': scaler_critic.state_dict(),\n",
    "                            'g_losses_history': g_losses_hist,\n",
    "                            'c_losses_history': c_losses_hist,\n",
    "                            'fid_scores_history': fid_scores_hist,\n",
    "                            'fid_epochs_history': fid_epochs_hist,\n",
    "                            'kid_scores_history': kid_scores_hist,\n",
    "                            'kid_std_history': kid_std_hist,\n",
    "                            'kid_epochs_history': kid_epochs_hist,\n",
    "                            'best_fid': best_fid,\n",
    "                            'best_kid': best_kid,\n",
    "                            'epochs_no_improve': epochs_no_improve\n",
    "                        }, BEST_KID_CHECKPOINT_FILE)\n",
    "                            \n",
    "                    # Handle early stopping based on primary metric\n",
    "                    if metric_improved:\n",
    "                        epochs_no_improve = 0\n",
    "                        # Always save the best model based on the primary metric\n",
    "                        save_best_model(generator)\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                        logger.info(f\"{PRIMARY_EVAL_METRIC} did not improve. Patience: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}.\")\n",
    "                        \n",
    "                        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                            logger.warning(f\"--- Early stopping triggered after {epochs_no_improve} epochs without {PRIMARY_EVAL_METRIC} improvement. ---\")\n",
    "                            stop_reason = f\"Early stopping ({PRIMARY_EVAL_METRIC})\"\n",
    "                            early_stop_triggered = True\n",
    "            \n",
    "            # --- Memory cleanup after FID/KID --- \n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # --- Checkpointing ---\n",
    "            checkpoint_state = {\n",
    "                'epoch': epoch + 1, \n",
    "                'step': global_step,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'critic_state_dict': critic.state_dict(),\n",
    "                'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                'scaler_gen_state_dict': scaler_gen.state_dict(),\n",
    "                'scaler_critic_state_dict': scaler_critic.state_dict(),\n",
    "                'g_losses_history': g_losses_hist,\n",
    "                'c_losses_history': c_losses_hist,\n",
    "                'fid_scores_history': fid_scores_hist,\n",
    "                'fid_epochs_history': fid_epochs_hist,\n",
    "                'kid_scores_history': kid_scores_hist,\n",
    "                'kid_std_history': kid_std_hist,\n",
    "                'kid_epochs_history': kid_epochs_hist,\n",
    "                'best_fid': best_fid,\n",
    "                'best_kid': best_kid,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }\n",
    "            \n",
    "            # Save checkpoint every N epochs or if it's the last epoch\n",
    "            if (epoch + 1) % CHECKPOINT_FREQ_EPOCHS == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "                save_checkpoint(checkpoint_state, f\"checkpoint_epoch_{epoch+1:04d}_sn_v2151.pth.tar\")\n",
    "                \n",
    "            # Always save the latest checkpoint\n",
    "            save_checkpoint(checkpoint_state, CHECKPOINT_FILE) \n",
    "            \n",
    "            # Check for early stopping\n",
    "            if early_stop_triggered:\n",
    "                break\n",
    "\n",
    "        # --- End of Epoch Loop ---\n",
    "\n",
    "        # --- Training Finished ---\n",
    "        total_training_time = time.time() - training_start_time\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        if not early_stop_triggered:\n",
    "            stop_reason = f\"Completed {NUM_EPOCHS} epochs\"\n",
    "            \n",
    "        training_successful = True\n",
    "        logger.info(\"--- Training Finished Successfully ---\")\n",
    "        logger.info(f\"Reason: {stop_reason}\")\n",
    "        logger.info(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "        logger.info(f\"Final Global Step: {global_step}\")\n",
    "        \n",
    "        if fid_scores_hist:\n",
    "            logger.info(f\"Best FID Score Achieved: {best_fid:.4f}\")\n",
    "        if kid_scores_hist:\n",
    "            logger.info(f\"Best KID Score Achieved: {best_kid:.6f}\")\n",
    "            \n",
    "        # Generate final plots and visualizations\n",
    "        logger.info(\"Generating final visualization plots...\")\n",
    "        \n",
    "        # Use best features for visualization if available, otherwise use last epoch features\n",
    "        if best_fake_features is None and os.path.exists(BEST_FEATURES_PATH):\n",
    "            try:\n",
    "                best_fake_features = np.load(BEST_FEATURES_PATH)\n",
    "                logger.info(f\"Loaded best fake features for visualization from: {BEST_FEATURES_PATH}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load best fake features: {e}\")\n",
    "        \n",
    "        generate_all_visualizations(\n",
    "            g_losses_hist, c_losses_hist, \n",
    "            fid_scores_hist, fid_epochs_hist, \n",
    "            kid_scores_hist, kid_std_hist, kid_epochs_hist,\n",
    "            real_features=final_real_features,\n",
    "            fake_features=best_fake_features,  # Use best features for visualization\n",
    "            save_dir=PLOT_DIR\n",
    "        )\n",
    "        \n",
    "        # If we never saved the best model (no FID/KID improvement), save the final model\n",
    "        if not os.path.exists(os.path.join(OUTPUT_DIR, BEST_MODEL_FILE)):\n",
    "            logger.info(\"No best model saved during training. Saving final model...\")\n",
    "            save_best_model(generator)\n",
    "            \n",
    "        # Collect training info for report\n",
    "        training_info = {\n",
    "            'epochs': epoch + 1 if 'epoch' in locals() else 0,\n",
    "            'stop_reason': stop_reason,\n",
    "            'training_time': total_training_time,\n",
    "            'best_fid': best_fid if best_fid != float('inf') else None,\n",
    "            'best_kid': best_kid if best_kid != float('inf') else None\n",
    "        }\n",
    "            \n",
    "        # Generate markdown report\n",
    "        generate_markdown_report(g_losses_hist, c_losses_hist, fid_scores_hist, fid_epochs_hist, kid_scores_hist, kid_epochs_hist, training_info)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"--- Training Interrupted by User ---\")\n",
    "        stop_reason = \"Manual Interruption\"\n",
    "        \n",
    "        logger.warning(\"Attempting to save checkpoint and generate plots before exit...\")\n",
    "        \n",
    "        if 'generator' in locals() and 'critic' in locals() and 'g_losses_hist' in locals():\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch if 'epoch' in locals() else 0, \n",
    "                'step': global_step if 'global_step' in locals() else 0, \n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'critic_state_dict': critic.state_dict(),\n",
    "                'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                'scaler_gen_state_dict': scaler_gen.state_dict() if 'scaler_gen' in locals() else None,\n",
    "                'scaler_critic_state_dict': scaler_critic.state_dict() if 'scaler_critic' in locals() else None,\n",
    "                'g_losses_history': g_losses_hist,\n",
    "                'c_losses_history': c_losses_hist,\n",
    "                'fid_scores_history': fid_scores_hist,\n",
    "                'fid_epochs_history': fid_epochs_hist,\n",
    "                'kid_scores_history': kid_scores_hist,\n",
    "                'kid_std_history': kid_std_hist,\n",
    "                'kid_epochs_history': kid_epochs_hist,\n",
    "                'best_fid': best_fid,\n",
    "                'best_kid': best_kid,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }, CHECKPOINT_FILE)\n",
    "            \n",
    "            # Check if best features exist\n",
    "            if best_fake_features is None and os.path.exists(BEST_FEATURES_PATH):\n",
    "                try:\n",
    "                    best_fake_features = np.load(BEST_FEATURES_PATH)\n",
    "                    logger.info(f\"Loaded best fake features for visualization from: {BEST_FEATURES_PATH}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load best fake features: {e}\")\n",
    "            \n",
    "            # Generate plots with available data\n",
    "            generate_all_visualizations(\n",
    "                g_losses_hist, c_losses_hist, \n",
    "                fid_scores_hist, fid_epochs_hist, \n",
    "                kid_scores_hist, kid_std_hist, kid_epochs_hist,\n",
    "                real_features=final_real_features,\n",
    "                fake_features=best_fake_features,  # Use best features for visualization if available\n",
    "                save_dir=PLOT_DIR\n",
    "            )\n",
    "        \n",
    "    except Exception as main_e:\n",
    "        # Catch any unexpected error during setup or the main loop that wasn't handled inside\n",
    "        logger.critical(f\"Critical error during training setup or execution: {main_e}\", exc_info=True)\n",
    "        stop_reason = f\"Error: {str(main_e)}\"\n",
    "        \n",
    "        # Try to save checkpoint if models exist\n",
    "        if 'generator' in locals() and 'critic' in locals() and 'g_losses_hist' in locals():\n",
    "            logger.warning(\"Attempting to save checkpoint before exit...\")\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch if 'epoch' in locals() else 0, \n",
    "                'step': global_step if 'global_step' in locals() else 0, \n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'critic_state_dict': critic.state_dict(),\n",
    "                'optimizer_gen_state_dict': opt_gen.state_dict(),\n",
    "                'optimizer_critic_state_dict': opt_critic.state_dict(),\n",
    "                'scaler_gen_state_dict': scaler_gen.state_dict() if 'scaler_gen' in locals() else None,\n",
    "                'scaler_critic_state_dict': scaler_critic.state_dict() if 'scaler_critic' in locals() else None,\n",
    "                'g_losses_history': g_losses_hist,\n",
    "                'c_losses_history': c_losses_hist,\n",
    "                'fid_scores_history': fid_scores_hist,\n",
    "                'fid_epochs_history': fid_epochs_hist,\n",
    "                'kid_scores_history': kid_scores_hist,\n",
    "                'kid_std_history': kid_std_hist,\n",
    "                'kid_epochs_history': kid_epochs_hist,\n",
    "                'best_fid': best_fid,\n",
    "                'best_kid': best_kid,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }, CHECKPOINT_FILE)\n",
    "            \n",
    "            # Check if we have a generated sample to save\n",
    "            if 'fake_features_epoch' in locals() and fake_features_epoch is not None:\n",
    "                save_fake_features(fake_features_epoch)\n",
    "        \n",
    "    finally: \n",
    "        # This block ALWAYS runs, whether the try succeeded or an exception occurred\n",
    "        logger.info(\"--- Running Final Cleanup ---\")\n",
    "        \n",
    "        # Final memory usage report\n",
    "        log_gpu_memory_usage(\"Final\")\n",
    "        \n",
    "        # Cleanup NVML\n",
    "        if NVML_AVAILABLE:\n",
    "            try: pynvml.nvmlShutdown()\n",
    "            except: pass\n",
    "            \n",
    "        # Run analysis scripts after training is complete\n",
    "        logger.info(\"--- Running Analysis Scripts ---\")\n",
    "        try:\n",
    "            # Import analysis modules - only re-import if needed to avoid namespace conflicts\n",
    "            if 'plt' not in globals():\n",
    "                import matplotlib.pyplot as plt\n",
    "            import re\n",
    "            import glob\n",
    "            from PIL import Image\n",
    "            \n",
    "            # Log script execution\n",
    "            logger.info(f\"Starting training progress analysis...\")\n",
    "            \n",
    "            # If we didn't already generate visualizations, do it now\n",
    "            if 'g_losses_hist' in locals() and len(g_losses_hist) > 0 and 'c_losses_hist' in locals() and len(c_losses_hist) > 0:\n",
    "                # Collect training info for report\n",
    "                if 'training_info' not in locals():\n",
    "                    training_info = {\n",
    "                        'epochs': len(g_losses_hist),\n",
    "                        'stop_reason': stop_reason if 'stop_reason' in locals() else \"Unknown\",\n",
    "                        'training_time': total_training_time if 'total_training_time' in locals() else 0,\n",
    "                        'best_fid': best_fid if 'best_fid' in locals() and best_fid != float('inf') else None,\n",
    "                        'best_kid': best_kid if 'best_kid' in locals() and best_kid != float('inf') else None\n",
    "                    }\n",
    "                \n",
    "                # Try to load best features for visualization if not already in memory\n",
    "                if best_fake_features is None and os.path.exists(BEST_FEATURES_PATH):\n",
    "                    try:\n",
    "                        best_fake_features = np.load(BEST_FEATURES_PATH)\n",
    "                        logger.info(f\"Loaded best fake features for visualization from: {BEST_FEATURES_PATH}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Failed to load best fake features: {e}\")\n",
    "                \n",
    "                # Generate visualizations if not already done\n",
    "                generate_all_visualizations(\n",
    "                    g_losses_hist, c_losses_hist, \n",
    "                    fid_scores_hist, fid_epochs_hist, \n",
    "                    kid_scores_hist, kid_std_hist, kid_epochs_hist,\n",
    "                    real_features=final_real_features if 'final_real_features' in locals() else None,\n",
    "                    fake_features=best_fake_features if 'best_fake_features' in locals() else None,\n",
    "                    save_dir=PLOT_DIR\n",
    "                )\n",
    "                    \n",
    "                # Generate markdown report if not already generated\n",
    "                generate_markdown_report(\n",
    "                    g_losses_hist, c_losses_hist, \n",
    "                    fid_scores_hist, fid_epochs_hist, \n",
    "                    kid_scores_hist, kid_epochs_hist, \n",
    "                    training_info\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                logger.warning(\"No loss history available for analysis or visualizations already generated.\")\n",
    "                \n",
    "        except Exception as analysis_error:\n",
    "            logger.error(f\"Error during analysis script execution: {analysis_error}\", exc_info=True)\n",
    "        \n",
    "        logger.info(\"--- Shutting down logging ---\")\n",
    "        logging.shutdown()\n",
    "\n",
    "# ==============================================================================\n",
    "# --- End of Script ---\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d8d34-dd34-4b8f-8a76-6651b2dfcb82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
